{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60700da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "from torchaudio import transforms as T\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from torcheval.metrics import Mean\n",
    "from string import ascii_lowercase\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d095e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = [\"\", \"B\", \"E\"]\n",
    "n_ft = 300\n",
    "h_len = 400\n",
    "d_model = 240\n",
    "cnn_out = 240\n",
    "bs = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2a6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch=None):\n",
    "    model.train()\n",
    "    metric = Mean().to(device)\n",
    "    with tqdm(train_loader, unit='batch') as tepochs:\n",
    "        for wave, labels, sr in tepochs:\n",
    "            if epoch is not None:\n",
    "                tepochs.set_description(f'epoch:{epoch}')\n",
    "            yp = model(train_transform(wave.to(device)), labels.to(device)[:, :-1])\n",
    "            loss = loss_fn(yp.transpose(2, 1), labels.to(device)[:, 1:])\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(loss)\n",
    "\n",
    "            tepochs.set_postfix(loss=metric.compute().item())\n",
    "    return model, metric.compute().item()\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    metric = Mean().to(device)\n",
    "    with torch.no_grad():\n",
    "        for wave, labels, sr in test_loader:\n",
    "            yp = model(train_transform(wave.to(device)), labels.to(device)[:, :-1])\n",
    "            loss = loss_fn(yp.transpose(2, 1), labels.to(device)[:, 1:])\n",
    "            metric.update(loss)\n",
    "    print(metric.compute().item())\n",
    "    return metric.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68755427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(T.AmplitudeToDB()(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1823a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff2ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lower = lambda x: re.sub(r\"[^1-9a-z\\s']\", '', x.lower())\n",
    "vocab = build_vocab_from_iterator(\n",
    "    data['info'].apply(get_lower), special_first=True, specials=specials)\n",
    "vocab.set_default_index(vocab[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a73117b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'B',\n",
       " 'E',\n",
       " ' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'a',\n",
       " 'o',\n",
       " 'n',\n",
       " 'i',\n",
       " 's',\n",
       " 'r',\n",
       " 'h',\n",
       " 'd',\n",
       " 'l',\n",
       " 'c',\n",
       " 'f',\n",
       " 'u',\n",
       " 'm',\n",
       " 'w',\n",
       " 'p',\n",
       " 'g',\n",
       " 'b',\n",
       " 'y',\n",
       " 'v',\n",
       " 'k',\n",
       " 'x',\n",
       " 'q',\n",
       " 'j',\n",
       " \"'\",\n",
       " '1',\n",
       " '2',\n",
       " 'z',\n",
       " '3',\n",
       " '6',\n",
       " '9',\n",
       " '8',\n",
       " '5',\n",
       " '4',\n",
       " '7']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a573c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/anaconda3/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (80) may be set too high. Or, the value for `n_freqs` (151) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_transform = nn.Sequential(T.MelSpectrogram(n_fft=n_ft, hop_length=h_len, n_mels=80), T.FrequencyMasking(10), \n",
    "                                T.TimeMasking(10)).to(device)\n",
    "valid_transform = T.MelSpectrogram(n_fft=n_ft, hop_length=h_len, n_mels=80).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8cd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec25daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = data.iloc[10000:11500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbdb056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LJSpeechSet(Dataset):\n",
    "    def __init__(self, data, phase='train'):\n",
    "        \n",
    "        pathx = 'LJSpeech-1.1/wavs/'\n",
    "        self.data = dict()\n",
    "        self.wave_forms, self.labels, self.sample_rates = list(), list(), list()\n",
    "\n",
    "        for i, row in data.iterrows():\n",
    "\n",
    "            wave_form, s_r = torchaudio.load(pathx + str(row['f_name']))\n",
    "            label = torch.LongTensor([vocab[c] for c in \"B\"+\n",
    "                                      re.sub(r\"[^1-9a-z\\s']\", '', str(row['info'].lower()))+\"E\"])\n",
    "\n",
    "            self.wave_forms.append(wave_form.squeeze()), self.labels.append(label), self.sample_rates.append(s_r)\n",
    "\n",
    "\n",
    "        self.phase = phase\n",
    "        self.wave_forms = pad_sequence(self.wave_forms, batch_first=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wave_forms)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.wave_forms[ind], self.labels[ind], self.sample_rates[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e88ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LJSpeechSet(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca875ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_set = LJSpeechSet(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035c302a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 557])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform(train_set[0][0].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea79b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    mel_waves = pad_sequence([b[0] for b in batch], batch_first=True).unsqueeze(1)\n",
    "    labels = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=vocab[''])\n",
    "    rates = [b[2] for b in batch]\n",
    "    return mel_waves, labels, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1a738b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, bs, shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_set, bs, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e24da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = nn.Sequential(T.Spectrogram(n_fft=n_ft, hop_length=h_len), T.FrequencyMasking(20), \n",
    "                                T.TimeMasking(20)).to(device)\n",
    "valid_transform = T.Spectrogram(n_fft=n_ft, hop_length=h_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e753a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, out, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inp, out, kernel_size=kernel, padding=kernel//2)\n",
    "        self.conv2 = nn.Conv2d(out, out, kernel_size=kernel, padding=kernel//2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel, stride=stride, padding=kernel//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.batch_n = nn.BatchNorm2d(out)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        \n",
    "        x = self.relu(self.batch_n(self.conv1(x)))\n",
    "        x = self.relu(self.batch_n(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, output):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.block1 = Block(1, 8, 7, 4)\n",
    "        self.block2 = Block(8, 12, 5, 2)\n",
    "        self.block3 = Block(12, 16, 4, 2)\n",
    "        self.block4 = Block(16, 14, 3, 1)\n",
    "        self.output = nn.LazyLinear(output)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        # reshaping to bs, c*frames* bin which has strided for 8 times\n",
    "        # and using linear to increase bin features\n",
    "        x = x.view(x.size(0), -1, x.size(2))\n",
    "        y = self.output(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcdb2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73a9a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, cnn_out):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), d_model, padding_idx=0)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0.1)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=cnn_out, nhead=6, dim_feedforward=cnn_out*4, \n",
    "                                               batch_first=True, activation='gelu')\n",
    "        \n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=8)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model, 6, d_model*4, batch_first=True, activation='gelu')\n",
    "        self.dec = nn.TransformerDecoder(dec_layer, 8)\n",
    "\n",
    "\n",
    "    def forward(self, x, inp):\n",
    "        tgt = self.embedding(inp)\n",
    "        y = self.enc(x)\n",
    "#         z = self.positional_encoding(tgt)\n",
    "        y = self.dec(tgt, y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cfe4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRNeural(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, cnn_out):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.cnn_ = CNNFeatureExtractor(cnn_out)\n",
    "        self.transformer = Transformer(d_model, cnn_out)\n",
    "        self.head = nn.Linear(d_model, len(vocab))\n",
    "        \n",
    "    def forward(self, x, inp):\n",
    "        y = self.cnn_(x)\n",
    "        y = self.transformer(y, inp)\n",
    "        y = self.head(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d270732b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ASRNeural(d_model, cnn_out).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5d6ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3babf628",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f35b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_hist = list()\n",
    "loss_valid_hist = list()\n",
    "pre_train_hist = list()\n",
    "pre_valid_hist = list()\n",
    "best_loss_valid = 1e+4\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6912e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0: 100%|██████████████████| 200/200 [01:22<00:00,  2.41batch/s, loss=3.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7860358079274494\n",
      "Model SAVED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1: 100%|██████████████████| 200/200 [01:21<00:00,  2.44batch/s, loss=3.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.930072816212972\n",
      "Model SAVED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:2:  48%|█████████▏         | 97/200 [00:40<00:43,  2.36batch/s, loss=2.91]"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "for epoch in range(n):\n",
    "    model, train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n",
    "    valid_loss = evaluate(model, valid_loader, loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_train_hist.append(train_loss)\n",
    "    loss_valid_hist.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_loss_valid:\n",
    "        torch.save(model,'modelx1.pt')\n",
    "        best_loss_valid =  valid_loss\n",
    "        print('Model SAVED') \n",
    "\n",
    "    epoch_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d485ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave, label, sr = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb720f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generated = [vocab['B']]\n",
    "\n",
    "for i in range(90):\n",
    "    with torch.no_grad():\n",
    "        preded = model(valid_transform(wave[0].to(device)).unsqueeze(0), \n",
    "                       torch.LongTensor(generated).to(device).unsqueeze(0))\n",
    "    argm = torch.multinomial((preded.squeeze(0) / .4).softmax(-1), 1)[-1]\n",
    "    if argm.item() == vocab['E']:\n",
    "        print('I have predicted the last item bro i cant take it more')\n",
    "        break\n",
    "    generated.append(argm.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a52b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef374d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''.join([itos[d] for d in torch.unique_consecutive(torch.LongTensor(generated))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef0286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
